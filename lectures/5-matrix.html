<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Matrix Algebra</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Matrix Algebra

---


## Last time

Correlations, covariances

---

## Matrix algebra

### This is statistics, not algebra

- Underlying mathematics of statistics includes a lot of algebra and calculus
  - Many textbooks and academic methods papers will use matrix algebra to explain statistical concepts

- Goal of this class: understand **why** 
   - Understand how statistics are related to each other, what factors can change your results, what's going on under the hood

--

- Another goal: find the framework for statistics that makes sense to you
  
---

## Matrix

A *matrix* is a rectangular array of numbers with n rows and m columns. It is symbolized with a bold, upper case letter, and subscripted to indicate its order.


 
`$$\LARGE \mathbf{G_{3,2}} = \left(\begin{array}
{rrr}
g_{11} &amp; g_{12}  \\
g_{21} &amp; g_{22} \\
g_{31} &amp; g_{32} 
\end{array}\right)$$`



The individual elements in a matrix are called *scalars*, subscripted to indicated their position in the matrix.

---

### Vectors

`$$\LARGE \mathbf{G_{3,2}} = \left(\begin{array}
{rrr}
g_{11} &amp; g_{12}  \\
g_{21} &amp; g_{22} \\
g_{31} &amp; g_{32} 
\end{array}\right)$$`



The columns in a matrix are called *vectors*, and are symbolized with lower case, bold letters. This matrix has two vectors (
`\(\mathbf{g}_1\)`
, 
`\(\mathbf{g}_2\)`
), each containing three scalars.

---

### Transpose




`$$\Large \mathbf{g_{3,1}} = \left(\begin{array}
{rrr}
g_{11}\\
g_{21}\\
g_{31} 
\end{array}\right)$$`



This vector is also a 3 x 1 matrix. When it is displayed as a row, it is symbolized differently:




`$$\Large \mathbf{g'_{1,3}} = \left(\begin{array}
{rrr}
g_{11} &amp; g_{12} &amp; g_{13} \\
\end{array}\right)$$`



This is called the *transpose* of the vector.


---

### Square vs rectangular
 
`$$\Large \mathbf{G_{3,2}} = \left(\begin{array}
{rrr}
g_{11} &amp; g_{12}  \\
g_{21} &amp; g_{22} \\
g_{31} &amp; g_{32} 
\end{array}\right)$$`

This matrix is *rectangular*. When the number of rows and columns are equal, the matrix is *square*:

`$$\Large \mathbf{F_{3,3}} = \left(\begin{array}
{rrr}
f_{11} &amp; f_{12} &amp; f_{13}  \\
f_{21} &amp; f_{22} &amp; f_{23} \\
f_{31} &amp; f_{32} &amp; f_{33} 
\end{array}\right)$$`


Data matrices are typically rectangular; correlation matrices and covariance matrices are always square.


---

### Symmetric



If `\(f_{i,j} = f_{j,i}\)` for all i and j, the matrix is *symmetric.*



`$$\Large \mathbf{F_{3,3}} = \left(\begin{array}
{rrr}
1 &amp; 3 &amp; -4  \\
3 &amp; 11 &amp; 7 \\
-4 &amp; 7 &amp; 2  
\end{array}\right)$$`

Correlation matrices and covariance matrices are symmetric.

---

### Diagonal



If all elements of a symmetric matrix except the main diagonal are zero, the matrix is a *diagonal* matrix:



`$$\Large \mathbf{F_{3,3}} = \left(\begin{array}
{rrr}
1 &amp; 0 &amp; 0  \\
0 &amp; 11 &amp; 0 \\
0 &amp; 0 &amp; 2  
\end{array}\right)$$`



`\(\mathbf{F}\)` is a symmetric, diagonal matrix with a trace of 14. 

---
##Matrix Addition and Subtraction:

- Matrices of the same order can be added and subtracted.

- These operations take place element by element.

.pull-left[
`$$\large \mathbf{F_{3,2}} = \left(\begin{array}
{rrr}
1 &amp; \underline{3}  \\
3 &amp; 11  \\
-4 &amp; 7   
\end{array}\right)$$`
]
.pull-right[
`$$\large \mathbf{H_{3,2}} = \left(\begin{array}
{rrr}
4 &amp; \underline{-1}  \\
6 &amp; 2  \\
12 &amp; 8   
\end{array}\right)$$`
]




`$$\mathbf{F_{3,2}} + \mathbf{H_{3,2}} = \mathbf{K_{3,2}} = \left(\begin{array}
{rrr}
1+4 &amp; \underline{3+-1}  \\
3+6 &amp; 11+2  \\
-4+12 &amp; 7+8   
\end{array}\right) = 
\left(\begin{array}
{rrr}
5 &amp; \underline{2}  \\
9 &amp; 13  \\
8 &amp; 15   
\end{array}\right)$$`

---

.pull-left[
`$$\large \mathbf{F_{3,2}} = \left(\begin{array}
{rrr}
1 &amp; \underline{3}  \\
3 &amp; 11  \\
-4 &amp; 7   
\end{array}\right)$$`
]
.pull-right[
`$$\large \mathbf{H_{3,2}} = \left(\begin{array}
{rrr}
4 &amp; \underline{-1}  \\
6 &amp; 2  \\
12 &amp; 8   
\end{array}\right)$$`
]




`$$\mathbf{F_{3,2}} - \mathbf{H_{3,2}} = \mathbf{K_{3,2}} = \left(\begin{array}
{rrr}
1-4 &amp; \underline{3--1}  \\
3-6 &amp; 11-2  \\
-4-12 &amp; 7-8   
\end{array}\right) = 
\left(\begin{array}
{rrr}
-3 &amp; \underline{4}  \\
-3 &amp; 9  \\
-16 &amp; -1   
\end{array}\right)$$`

???

Matrices must have the same dimensions to be added/subtracted
---

### Properties of addition and subtraction


**Matrix addition is commutative and associative:**

`$$\large \mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}$$`

`$$\large \mathbf{A} + \mathbf{B} + \mathbf{C} = (\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C})$$`

**Matrix subtraction is distributive:**

`$$\large \mathbf{A} – (\mathbf{B} + \mathbf{C}) = \mathbf{A} – \mathbf{B} – \mathbf{C}$$` 

`$$\large \mathbf{A} – (\mathbf{B} – \mathbf{C}) = \mathbf{A} – \mathbf{B} + \mathbf{C}$$`
---

### Null matrix


A matrix of zeros is called a *zero matrix* or a *null matrix*. It is used in solving equations:



`$$\small \left(\begin{array}
{rrr}
5 &amp; 7  \\
4 &amp; 2  \\
6 &amp; 1   
\end{array}\right)
+
\left(\begin{array}
{rrr}
y_{11} &amp; y_{12} \\
y_{21} &amp; y_{22} \\
y_{31} &amp; y_{32}
\end{array}\right) 
= 
\left(\begin{array}
{rrr}
7 &amp; -1 \\
7 &amp; 4 \\
8 &amp; 3
\end{array}\right)$$`



`$$\small \left(\begin{array}
                {rrr}
                -5 &amp; -7  \\
                -4 &amp; -2  \\
                -6 &amp; -1   
                \end{array}\right)
+
  \left(\begin{array}
         {rrr}
         5 &amp; 7  \\
         4 &amp; 2  \\
         6 &amp; 1   
         \end{array}\right)
+
  \left(\begin{array}
         {rrr}
         y_{11} &amp; y_{12} \\
         y_{21} &amp; y_{22} \\
         y_{31} &amp; y_{32}
         \end{array}\right) 
= 
    \left(\begin{array}
           {rrr}
           -5 &amp; -7  \\
           -4 &amp; -2  \\
           -6 &amp; -1   
           \end{array}\right)
  +
    \left(\begin{array}
           {rrr}
           7 &amp; -1 \\
           7 &amp; 4 \\
           8 &amp; 3
           \end{array}\right)$$`
    
`$$\small \left(\begin{array}
{rrr}
0 &amp; 0  \\
0 &amp; 0  \\
0 &amp; 0   
\end{array}\right)
+
\left(\begin{array}
{rrr}
y_{11} &amp; y_{12} \\
y_{21} &amp; y_{22} \\
y_{31} &amp; y_{32}
\end{array}\right) 
= 
\left(\begin{array}
{rrr}
2 &amp; -8 \\
3 &amp; 2 \\
2 &amp; 2
\end{array}\right)$$`

---

### Scalar multiplication

Scalar multiplication is accomplished by multiplying every element of a matrix by a constant scalar:

.pull-left[
`$$\large \mathbf{F_{3,3}} = \left(\begin{array}
{rrr}
1 &amp; 3 &amp; -4  \\
3 &amp; 11 &amp; 7 \\
-4 &amp; 7 &amp; 2  
\end{array}\right)$$`
]

.pull-right[
`$$\large k = 5$$`
]


`$$\large k\mathbf{F} = \left(\begin{array}
{rrr}
1\times5 &amp; 3\times5 &amp; -4\times5  \\
3\times5 &amp; 11\times5 &amp; 7\times5 \\
-4\times5 &amp; 7\times5 &amp; 2\times5  
\end{array}\right)$$`

---

### Properties of multiplication

Matrix multiplication requires attending to a few important rules:

- The order of multiplication is important.

- Matrices can only be multiplied if the number of columns of the first matrix is equal to the number of rows of the second matrix.

- The resulting matrix has an order equal to the number of rows of the first matrix and the number of columns of the second matrix.

---

`$$\LARGE \mathbf{A}_{j,i} \times \mathbf{B}_{i,k} = \mathbf{C}_{j,k}$$`

The elements of C are defined as:

- the scalar in the *j*th row and *k*th column of `\(\mathbf{C}\)` is the sum of the scalar products of the *j*th row of `\(\mathbf{A}\)`  and the *k*th column of `\(\mathbf{B}\)`.

Or

`$$\LARGE \displaystyle\sum_{n=1}^{i}{a_{j,n}\times b_{n,k}}$$`
---


.pull-left[
`$$\large \mathbf{A_{3,2}} = \left(\begin{array}
{rrr}
4 &amp; -1\\
7 &amp; 0 \\
6 &amp; 2  
\end{array}\right)$$`
]

.pull-right[

`$$\large \mathbf{B_{2,3}} = \left(\begin{array}
{rrr}
1 &amp; 7 &amp; 0 \\
4 &amp; 2 &amp; 6 
\end{array}\right)$$`

]



`$$\large \mathbf{A}_{3,2}\times\mathbf{B}_{2,3} = \mathbf{C}_{3,3}$$`


.pull-right[
`$$\large \mathbf{C}_{3,3} = \left(\begin{array}
{rrr}
0 &amp; 26 &amp; -6   \\
7 &amp; 49 &amp; 0 \\
14 &amp; 46 &amp; 12
\end{array}\right)$$`
]


.pull-left[
`$$\large (4 \times 1) + (-1 \times 4) = 0$$`
`$$\large (7 \times 7) + (0 \times 2) = 49$$`
]

---

### Matrix multiplication is not commutative

- It may not always be possible

  - `\(\Large \mathbf{A}_{2,3}\mathbf{B}_{3,5} = \mathbf{C}_{2,5}\)` but `\(\Large \mathbf{B}_{3,5}\mathbf{A}_{2,3}\)` is impossible.
  
- When possible, the results may not be equal:
  - `\(\Large \mathbf{A}_{1,3}\mathbf{B}_{3,1} = \mathbf{C}_{1,1}\)` but `\(\Large \mathbf{B}_{3,1}\mathbf{A}_{1,3} = \mathbf{C}_{3,3}\)`
  
---

### Properties of multiplication

**Matrix multiplication is associative:**

`$$\Large \mathbf{ABC} = \mathbf{(AB)C} = \mathbf{A(BC)}$$`

**Matrix multiplication is distributive:** 

`$$\Large \mathbf{A(B+C)} = \mathbf{AB} + \mathbf{AC}$$`

**But order is important:**

`$$\Large \mathbf{XA} + \mathbf{BX} \neq \mathbf{X(A + B)}$$`
  
  
---
### Transpose


Every matrix has a *transpose* that is obtained by exchanging the rows and columns:

`$$\large \mathbf{X} = \left(\begin{array}
{rrr}
4 &amp; -1    \\
7 &amp; 0  \\
6 &amp; 2 
\end{array}\right)$$`



`$$\large \mathbf{X'} = \left(\begin{array}
{rrr}
4 &amp; 7 &amp; 6    \\
-1 &amp; 0 &amp; 2  \\
\end{array}\right)$$`



---

###Transpose multiplication



Transposes are useful for arranging a matrix so that matrix multiplication is possible.


Example: A common statistical requirement is to generate the sums of squares and cross- products for a data matrix. If `\(\large \mathbf{X_{n,v}}\)` is a matrix of scores, then `\(\large \mathbf{X_{nv}X_{nv}}\)` is not possible. 



But, `\(\large \mathbf{X'_{vn}X_{nv}}\)` can be carried out.

 - Also useful: the transpose of a product of matrices is equal to the product of the transposed matrices in reverse order
 `$$(\mathbf{XY})' = \mathbf{Y'X'}$$`

???

**Do students recognize what this matrix multiplication is trying to do?**

---

`\(\mathbf{D}\)` is a matrix of deviation scores for 5 individuals on 2 variables.

.pull-left[
`$$\mathbf{D} = \left(\begin{array}
{rrr}
d_{11} &amp; d_{12}    \\
d_{21} &amp; d_{22}    \\
d_{31} &amp; d_{32}    \\
d_{41} &amp; d_{42}    \\
d_{51} &amp; d_{52}    
\end{array}\right)$$`
]

.pull-right[
`$$\mathbf{D'} = \left(\begin{array}
{rrr}
d_{11} &amp; d_{12} &amp; d_{13} &amp; d_{14} &amp; d_{15}   \\
d_{21} &amp; d_{22} &amp; d_{23} &amp; d_{24} &amp; d_{25}   \\
\end{array}\right)$$`
]

`$$\Large
\mathbf{D'D} = ??$$`

---

`\(\mathbf{D}\)` is a matrix of deviation scores for 5 individuals on 2 variables.

.pull-left[
`$$\mathbf{D} = \left(\begin{array}
{rrr}
d_{11} &amp; d_{12}    \\
d_{21} &amp; d_{22}    \\
d_{31} &amp; d_{32}    \\
d_{41} &amp; d_{42}    \\
d_{51} &amp; d_{52}    
\end{array}\right)$$`
]

.pull-right[
`$$\mathbf{D'} = \left(\begin{array}
{rrr}
d_{11} &amp; d_{12} &amp; d_{13} &amp; d_{14} &amp; d_{15}   \\
d_{21} &amp; d_{22} &amp; d_{23} &amp; d_{24} &amp; d_{25}   \\
\end{array}\right)$$`
]

`$$\Large
\mathbf{D'D} = \left(\begin{array}
{rr}
x_{11} &amp; x_{12} \\
x_{21} &amp; x_{22}
\end{array}\right)$$`

--

`\(\large x_{11}\)` and `\(\large x_{22}\)` are sums of squared deviations.

--

`\(\large x_{12}\)` and `\(\large x_{21}\)` are sums of cross products of deviations.

--

What matrix is `\(\mathbf{D'D}\)` one step away from?

???
Answer: variance/covariance matrix!

**How do we get there?**

Divide by `\(N-1\)`!

---

Multiplication by diagonal matrices is especially important in statistics and is used to accomplish rescaling (expanding, shrinking, standardizing).


Post-multiplication of a matrix X by a diagonal matrix `\(\mathbf{D}\)` results in the *columns* of `\(\mathbf{X}\)` being multiplied by the corresponding diagonal element in `\(\mathbf{D}\)`.


.pull-left[
`$$\Large \mathbf{X} = \left(\begin{array}
{rr}
4 &amp; 7 \\
7 &amp; 6
\end{array}\right)$$`
]
.pull-right[
`$$\Large \mathbf{D} = \left(\begin{array}
{rr}
3 &amp; 0 \\
0 &amp; 4
\end{array}\right)$$`
]



`$$\Large \mathbf{XD} = \mathbf{Y} = \left(\begin{array}
{rr}
12 &amp; 28 \\
21 &amp; 24
\end{array}\right)$$`



The first column of X is multiplied by 3; the second column is multiplied by 4.

---

Pre-multiplication of a matrix X by a diagonal matrix `\(\mathbf{D}\)` results in the *rows* of `\(\mathbf{X}\)` being multiplied by the corresponding diagonal element in `\(\mathbf{D}\)`.



.pull-left[
`$$\Large \mathbf{D} = \left(\begin{array}
{rr}
3 &amp; 0 \\
0 &amp; 2
\end{array}\right)$$`
]
.pull-right[
`$$\Large \mathbf{X} = \left(\begin{array}
{rr}
4 &amp; 7 \\
7 &amp; 6
\end{array}\right)$$`
]



`$$\Large \mathbf{DX} = \mathbf{Y} = \left(\begin{array}
{rr}
12 &amp; 21 \\
14 &amp; 12
\end{array}\right)$$`



The first row of X is multiplied by 3; the second row is multiplied by 2.

---

### Scalar multiplication (again)



Scalar multiplication is just multiplication by a diagonal matrix with a constant in the diagonal.



`$$\Large \left(\begin{array}
{rr}
3 &amp; 0 \\
0 &amp; 3
\end{array}\right)
\left(\begin{array}
{rr}
4 &amp; 7 \\
7 &amp; 6
\end{array}\right)
= 
\left(\begin{array}
{rr}
12 &amp; 21 \\
21 &amp; 18
\end{array}\right)$$`



`$$\Large \left(\begin{array}
{rr}
4 &amp; 7 \\
7 &amp; 6
\end{array}\right)
\left(\begin{array}
{rr}
3 &amp; 0 \\
0 &amp; 3
\end{array}\right)
=
\left(\begin{array}
{rr}
12 &amp; 21 \\
21 &amp; 18
\end{array}\right)$$`



---
### Identity matrix


The *identity matrix* is a diagonal matrix with ones on the main diagonal:



`$$\Large \mathbf{I} = \left(\begin{array}
{rrrr}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{array}\right)$$`



The identity matrix is often a useful target matrix in statistics.

---

### Inverse matrix

Some square matrices have an *inverse* such that `\(\large \mathbf{AA^{-1}} = I\)`. The inverse is useful in solving matrix equations:

`$$\Large \mathbf{Y} = \mathbf{BR}$$`

Solving for B:

`$$\Large \mathbf{YR^{-1}} = \mathbf{BRR^{-1}}$$`
`$$\Large \mathbf{YR^{-1}} = \mathbf{B}$$`
---

class: inverse

## Statistics through matrix algebra

---

## Linear combinations

Perhaps **the most important rules of matrix algebra** to remember are that post-multiplication transforms columns and pre-multiplication transforms rows.

By applying pre- and post-multiplication to a dataframe of raw scores you can perform transformations to your raw data -- including calculating sum or weighted scores -- and aggregate your data into whatever groups or configurations are necessary.

---

##Linear combinations

If I create the following linear combinations of the original variables:

.pull-left[
`$$\large \mathbf{X} = \left(\begin{array}
{rrr}
X_{11} &amp; X_{12} &amp; X_{13} \\
X_{21} &amp; X_{22} &amp; X_{23} \\
X_{31} &amp; X_{32} &amp; X_{33} \\
\vdots &amp; \vdots &amp; \vdots \\
X_{n1} &amp; X_{n2} &amp; X_{n3} 
\end{array}\right)$$`
]


.pull-right[
`$$\large \mathbf{T} = \left(\begin{array}
{rrr}
1 &amp; -1 &amp; 1 \\
1 &amp; 0 &amp; -2 \\
1 &amp; 1 &amp; 1
\end{array}\right)$$`
]

`$$\Large \mathbf{XT} = Y$$`

What do the new variables represent? What are the variances and covariances of the new variables?

???

`\(\mathbf{Y}\)` is a `\(n x 3\)` matrix. 

Post-multiplcation = affecting the columns.


Each row of Y corresponds to one person.
First column is the sum of the person's values (e.g., sum all values in a scale)
Second column = 3rd-1st
Third column = (1st+3rd)-2*2nd

---

##Linear combinations

If I add a linear combinations as premultiplication:

.pull-left[
`$$\large \mathbf{T} = \left(\begin{array}
{rr}
1 &amp; 1 \\
1 &amp; 1 \\
1 &amp; 1 \\
\vdots &amp; \vdots \\
0 &amp; -1 \\ 
0 &amp;  -1\\
\end{array}\right)$$`
]


.pull-right[
`$$\large \mathbf{X} = \left(\begin{array}
{rrr}
X_{11} &amp; X_{12} &amp; X_{13} \\
X_{21} &amp; X_{22} &amp; X_{23} \\
X_{31} &amp; X_{32} &amp; X_{33} \\
\vdots &amp; \vdots &amp; \vdots \\
X_{n1} &amp; X_{n2} &amp; X_{n3} 
\end{array}\right)$$`
]

`$$\Large \mathbf{TX} = Y$$`

What do the new variables represent? What are the variances and covariances of the new variables?

???

`\(\mathbf{Y}\)` is a `\(2 x 3\)` matrix. 

Pre-multiplcation = affecting the rows


Each column of Y corresponds to one variable
First row is the mean of group 1
2nd row is difference in means of group 1 and group 2

---

Everything we need to know about the variances and covariances of the linear combinations is contained in the variance-covariance matrix of the original variables (
`\(\Sigma\)`
).



`$$\Large \Sigma = \left(\begin{array}
{rrr}
\sigma^2_{1} &amp; \sigma^2_{12} &amp; \sigma^2_{13} \\
\sigma^2_{21} &amp; \sigma^2_{2} &amp; \sigma^2_{23} \\
\sigma^2_{31} &amp; \sigma^2_{32} &amp; \sigma^2_{3} \\
\end{array}\right)$$`



If `\(\large \mathbf{Y}=\mathbf{XT}\)`,then `\(\large \Sigma_Y =\mathbf{T'\Sigma T}\)`

---

## Descriptive statistics

We can calculate the mean of a vector `\(\mathbf{x}\)` with length `\(n\)` using linear combinations:


.pull-left[
$$ \mathbf{T} = \left(\begin{array}
{r}
\frac{1}{n}  \\
\frac{1}{n}  \\
\frac{1}{n}  \\
\vdots \\
\frac{1}{n}  \\ 
\frac{1}{n} \\
\end{array}\right)$$`
]


.pull-right[
`$$\mathbf{X} = \left(\begin{array}
{rrr}
X_{11} &amp; X_{12} &amp; X_{13} \\
X_{21} &amp; X_{22} &amp; X_{23} \\
X_{31} &amp; X_{32} &amp; X_{33} \\
\vdots &amp; \vdots &amp; \vdots \\
X_{n1} &amp; X_{n2} &amp; X_{n3} 
\end{array}\right)$$`
]

This happens to be equivalent to the following formula:

`$$\Large \bar{X} = 1'\mathbf{X}(1'1)^{-1} = 1'\mathbf{X}\frac{1}{n}$$`


---
&lt;!-- ###Mean --&gt;

&lt;!-- Let's show this with a vector `\(\mathbf{x}\)` where `\(\large \mathbf{x'} = [8, 2, 9]\)` --&gt;

&lt;!-- `$$\large \bar{x} = 1'\mathbf{x}(1'1)^{-1}$$` --&gt;


&lt;!-- $$\large \bar{x} = \left[\begin{array} --&gt;
&lt;!-- {rrr} --&gt;
&lt;!-- 1 &amp; 1 &amp; 1 \\\end{array}\right] --&gt;
&lt;!-- \left[\begin{array} --&gt;
&lt;!-- {r} --&gt;
&lt;!-- 8 \\ --&gt;
&lt;!-- 2 \\ --&gt;
&lt;!-- 9 \end{array}\right](\left[\begin{array} --&gt;
&lt;!-- {rrr} --&gt;
&lt;!-- 1 &amp; 1 &amp; 1 \\\end{array}\right] --&gt;
&lt;!-- \left[\begin{array} --&gt;
&lt;!-- {r} --&gt;
&lt;!-- 1 \\ --&gt;
&lt;!-- 1 \\ --&gt;
&lt;!-- 1 \end{array}\right])^{-1}$$ --&gt;

&lt;!-- -- --&gt;


&lt;!-- $$\large \bar{x} = \left[\begin{array} --&gt;
&lt;!-- {r} --&gt;
&lt;!-- 19 \end{array}\right](\left[\begin{array} --&gt;
&lt;!-- {r} --&gt;
&lt;!-- 3\end{array}\right])^{-1}$$ --&gt;

&lt;!-- Now we find the inverse of the matrix `\([3]\)` such that this matrix times `\([3]\)` is equal to 1.  --&gt;

&lt;!-- -- --&gt;

&lt;!-- $$\large \bar{x} = \left[\begin{array} --&gt;
&lt;!-- {r} --&gt;
&lt;!-- 19 \end{array}\right](\left[\begin{array} --&gt;
&lt;!-- {r} --&gt;
&lt;!-- \frac{1}{3}\end{array}\right]) = 6.33$$ --&gt;

&lt;!-- ??? --&gt;

&lt;!-- Using [ instead of ( so it's easier to read. You'll see both.  --&gt;

&lt;!-- --- --&gt;

&lt;!-- ### Column means --&gt;

&lt;!-- We can apply almost the same formula to an `\(n \times c\)` matrix to find the means of each column.  --&gt;

&lt;!-- `$$\Large \bar{X_C} = 1'\mathbf{X}(1'1)^{-1} = 1'\mathbf{X}\frac{1}{n}$$` --&gt;

---

### Deviation scores

Any vector of `\(N\)` raw scores can be converted into deviation score form by pre-multiplication by a "projection operator" `\(\mathbf{Q}\)`. Diagonal elements of `\(\mathbf{Q}\)` are always equal to `\((N − 1)/N\)` , and off-diagonal elements are always equal to 
`\(−1/N\)`.

---

### Deviation scores

Imagine we have a vector of scores, `\(\mathbf{x} = [4, 2, 0]\)`. Our projection operator would then be

`$$\small \mathbf{Q} = \left(\begin{array}
{rrr}
\frac{2}{3} &amp; -\frac{1}{3} &amp; -\frac{1}{3} \\
-\frac{1}{3} &amp; \frac{2}{3} &amp; -\frac{1}{3} \\
-\frac{1}{3} &amp; -\frac{1}{3} &amp; \frac{2}{3} \\
\end{array}\right)$$`

And our deviation scores are

`$$\small \mathbf{D} = \mathbf{Qx} = \left(\begin{array}
{rrr}
\frac{2}{3} &amp; -\frac{1}{3} &amp; -\frac{1}{3} \\
-\frac{1}{3} &amp; \frac{2}{3} &amp; -\frac{1}{3} \\
-\frac{1}{3} &amp; -\frac{1}{3} &amp; \frac{2}{3} \\
\end{array}\right)
\left(\begin{array}
{r}
4 \\
2 \\
0 \\
\end{array}\right) = 
\left(\begin{array}
{r}
2 \\
0 \\
-2 \\
\end{array}\right)$$`

???

A conclusion of this is that a deviation score is always a linear combination of all the elements of its column, such that

`$$dev_i = \frac{n-1}{n}x_i-\Sigma\frac{1}{n}x_{\neg i}$$`

Does this make sense?

---

### Sample variance

We already know that sample variance is the sum of the squared devaition scores divided by `\(N-1\)`, so we can write this as a matrix algebra formula:

`$$\large S^2_{\mathbf{x}} = \frac{1}{N-1}\mathbf{D'D}$$`

We can then substitute our formula for the devaition score:

`$$\large S^2_{\mathbf{x}} = \frac{1}{N-1}\mathbf{(Qx)'(Qx)}$$`

The transpose of a product of matrices is equal to the product of the transposes in reverse order, so:

`$$\large S^2_{\mathbf{x}} = \frac{1}{N-1}\mathbf{x'Q'Qx}$$`
???
Note that S refers to the matrix of all variables and s refers to a vector for a specific variable.
---

### Variance/covariance matrix

Here's one more weird thing. It turns out that `\(\mathbf{Q'Q} = \mathbf{Q}\)`. (This is a proof we won't get into.) 

`$$\large S^2_{\mathbf{x}} = \frac{1}{N-1}\mathbf{x'Q'Qx} = \frac{1}{N-1}\mathbf{x'Qx}$$`



We can generalize this formula to:

`$$\large S_{\mathbf{xy}} = \frac{1}{N-1}\mathbf{x'Qy}$$`

???
Students should recognize covariance formula.

---
### Correlation matrix, `\(\large \mathbf{R}\)`

A correlation is simply the covariance divided by the standard deviations of the two variables, so we can create a new formula to calculate that.

Let `\(\mathbf{D_{xx}}\)` be a diagonal matrix with the diagonal value being `\(1/s_i\)` or the inverse of the standard deviation of that variable. Then we can calculate `\(\mathbf{R}\)` as

`$$\large \mathbf{R} = \mathbf{D_{xx}S_{XY}D_{xx}}$$`
---

## Matrix Algebra in statistics

Certainly matrix algebra takes relatively simple calculations and makes them more complicated...

... but for some statistics, the matrix algebra version is actually much simpler. 



---

class: inverse

## Next time...

probability!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
